import argparse
import os

import gymnasium as gym
import matplotlib.pyplot as plt
import numpy as np
import torch
from gymnasium.wrappers import RecordVideo
from stable_baselines3 import A2C
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor


class LoggingCallback(BaseCallback):
    """ì½œë°±ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê°’ë“¤ì„ ë¡œê¹…í•˜ëŠ” ì½œë°±"""

    def __init__(self, log_interval=1, verbose=0):
        super().__init__(verbose)
        self.log_interval = log_interval
        self.step_count = 0
        self.episode_count = 0
        self.episode_reward = 0.0  # ì—í”¼ì†Œë“œ ì´ ë³´ìƒ ì¶”ì 
        self.start_exploration_rate = False  # ì„±ê³µ ê¸°ì¤€ì  í”Œë˜ê·¸
        self.initial_exploration_rate = 1.0  # ì´ˆê¸° íƒí—˜ë¥ 

    def _on_step(self) -> bool:
        self.step_count += 1

        # ë²¡í„°í™”ëœ í™˜ê²½ì—ì„œëŠ” dones ë°°ì—´ ì‚¬ìš©
        dones = self.locals.get("dones", [False])

        # ì—í”¼ì†Œë“œê°€ ëë‚¬ëŠ”ì§€ í™•ì¸ (ë²¡í„°í™”ëœ í™˜ê²½)
        if dones[0]:  # ì²« ë²ˆì§¸ í™˜ê²½ì˜ done ìƒíƒœ
            self.episode_count += 1

            # infosì—ì„œ ì—í”¼ì†Œë“œ ì •ë³´ ì¶”ì¶œ
            episode_info = self.locals.get("infos", [{}])[0].get("episode", {})
            episode_reward = episode_info.get("r", 0.0)  # ì´ ë³´ìƒ
            episode_length = episode_info.get("l", 0)  # ì—í”¼ì†Œë“œ ê¸¸ì´
            episode_time = episode_info.get("t", 0.0)  # ì†Œìš” ì‹œê°„

            if self.episode_count % self.log_interval == 0:
                print(
                    f"ğŸ” ì½œë°± ë¡œê¹… - ì—í”¼ì†Œë“œ {self.episode_count}, steps: {self.step_count}, reward: {episode_reward}"
                )

        return True


def create_env(video_folder=None):
    """CartPole-v1 í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if video_folder:
        env = gym.make("CartPole-v1", render_mode="rgb_array")
        env = RecordVideo(env, video_folder)
        print(f"ë¹„ë””ì˜¤ ë…¹í™”ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤: {video_folder}")
        return env

    env = gym.make("CartPole-v1")
    # env = Monitor(env)
    return env


def setup_logging():
    """ë¡œê·¸ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    return log_dir


def setup_model_save_dir():
    """ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    save_dir = "zips"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    return save_dir


def create_a2c_model(env, log_dir="logs"):
    """A2C ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("A2C ëª¨ë¸ ìƒì„±...")

    model = A2C(
        "MlpPolicy",
        env,
        learning_rate=0.0007,
        n_steps=5,
        gamma=0.99,
        gae_lambda=1.0,
        ent_coef=0.0,
        vf_coef=0.25,
        max_grad_norm=0.5,
        rms_prop_eps=1e-05,
        use_rms_prop=True,
        use_sde=False,
        sde_sample_freq=-1,
        tensorboard_log=os.path.join(log_dir, "a2c"),
        policy_kwargs={
            "net_arch": [256, 256],
            "activation_fn": torch.nn.ReLU,
        },
        device="cuda",
    )

    return model


def evaluate_model(model, env, n_eval_episodes=10):
    """ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    mean_reward, std_reward = evaluate_policy(
        model, env, n_eval_episodes=n_eval_episodes
    )
    print(f"í‰ê·  ë³´ìƒ: {mean_reward:.2f} +/- {std_reward:.2f}")
    return mean_reward, std_reward


def test_model(model, env, n_episodes=5):
    """í›ˆë ¨ëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    print(f"\n{model.__class__.__name__} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    episode_rewards = []
    episode_steps = []
    clear_count = 0

    for episode in range(n_episodes):
        obs, _ = env.reset()
        done = False
        truncated = False
        total_reward = 0
        steps = 0

        while not (done or truncated):
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = env.step(action)
            total_reward += reward
            steps += 1

            if steps >= 500:  # CartPole-v1ì˜ ìµœëŒ€ ìŠ¤í…
                break

        episode_rewards.append(total_reward)
        episode_steps.append(steps)

        print(f"ì—í”¼ì†Œë“œ {episode + 1}: ì´ ë³´ìƒ = {total_reward}, ìŠ¤í… = {steps}")

        if total_reward >= 475:  # CartPole-v1 í´ë¦¬ì–´ ê¸°ì¤€ (500 ìŠ¤í… ì¤‘ 475 ì´ìƒ)
            print(f"ğŸ‰ ì—í”¼ì†Œë“œ {episode + 1}ì—ì„œ CartPole-v1ì„ í´ë¦¬ì–´í–ˆìŠµë‹ˆë‹¤!")
            clear_count += 1
        else:
            print(f"âŒ ì—í”¼ì†Œë“œ {episode + 1}ì—ì„œ í´ë¦¬ì–´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # í†µê³„ ìš”ì•½
    avg_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)
    clear_rate = clear_count / n_episodes * 100

    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    print(f"í‰ê·  ë³´ìƒ: {avg_reward:.2f} +/- {std_reward:.2f}")
    print(f"í´ë¦¬ì–´ìœ¨: {clear_rate:.1f}% ({clear_count}/{n_episodes})")
    print(f"ìµœê³  ë³´ìƒ: {max(episode_rewards)}")
    print(f"ìµœì € ë³´ìƒ: {min(episode_rewards)}")

    return episode_rewards, episode_steps


def run_a2c(no_save=False):
    """A2C ì•Œê³ ë¦¬ì¦˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    env = create_env()
    log_dir = setup_logging()
    save_dir = setup_model_save_dir()

    print(f"\n{'='*50}")
    print("A2C ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸")
    print(f"{'='*50}")

    # ëª¨ë¸ ìƒì„±
    model = create_a2c_model(env, log_dir)

    # í•™ìŠµ ì‹¤í–‰
    print("A2C ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    callback = LoggingCallback()
    model.learn(
        total_timesteps=20000,
        log_interval=100,
        callback=callback,
    )

    # í‰ê°€
    mean_reward, std_reward = evaluate_model(model, env, n_eval_episodes=20)

    # í…ŒìŠ¤íŠ¸
    test_model(model, env, n_episodes=3)

    # ëª¨ë¸ ì €ì¥
    if not no_save:
        model_path = os.path.join(save_dir, "cartpole_a2c")
        model.save(model_path)
        print(f"A2C ëª¨ë¸ì´ {model_path}.zipìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ëª¨ë¸ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    return mean_reward, std_reward


def play_a2c():
    """ì €ì¥ëœ A2C ëª¨ë¸ë¡œ ë¹„ë””ì˜¤ë¥¼ ì°ìŠµë‹ˆë‹¤."""
    save_dir = setup_model_save_dir()
    model_path = os.path.join(save_dir, "cartpole_a2c")

    # ëª¨ë¸ ë¡œë“œ
    try:
        model = A2C.load(model_path)
        print(f"ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {model_path}")
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œì£¼ì„¸ìš”.")
        return

    # ë¹„ë””ì˜¤ í´ë” ì„¤ì •
    video_folder = "videos/a2c"
    if not os.path.exists(video_folder):
        os.makedirs(video_folder)

    # ë¹„ë””ì˜¤ ë…¹í™” í™˜ê²½ ìƒì„±
    env = create_env(video_folder=video_folder)

    print(f"\n{'='*50}")
    print("A2C ëª¨ë¸ ë¹„ë””ì˜¤ ë…¹í™” ì‹œì‘")
    print(f"{'='*50}")

    # ë¹„ë””ì˜¤ ë…¹í™” ì‹¤í–‰
    obs, _ = env.reset()
    done = False
    truncated = False
    total_reward = 0.0
    steps = 0

    while not (done or truncated):
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, done, truncated, info = env.step(action)
        total_reward += float(reward)
        steps += 1

        if steps >= 500:  # CartPole-v1ì˜ ìµœëŒ€ ìŠ¤í…
            break

    env.close()

    print(f"ë¹„ë””ì˜¤ ë…¹í™” ì™„ë£Œ!")
    print(f"ì´ ë³´ìƒ: {total_reward}, ìŠ¤í…: {steps}")
    print(f"ë¹„ë””ì˜¤ íŒŒì¼ ìœ„ì¹˜: {video_folder}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser()
    parser.add_argument("video", nargs="?", default=None)
    parser.add_argument("--no-save", action="store_true")
    args = parser.parse_args()

    if args.video:
        play_a2c()
    else:
        mean_reward, std_reward = run_a2c(no_save=args.no_save)
        print(f"\nğŸ† A2C ìµœì¢… ê²°ê³¼: {mean_reward:.2f} +/- {std_reward:.2f}")


if __name__ == "__main__":
    main()
