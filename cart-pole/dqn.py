import argparse
import os

import gymnasium as gym
import matplotlib.pyplot as plt
import numpy as np
import torch
from gymnasium.wrappers import RecordVideo
from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor


class LoggingCallback(BaseCallback):
    """ì½œë°±ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê°’ë“¤ì„ ë¡œê¹…í•˜ëŠ” ì½œë°±"""

    def __init__(self, log_interval=1, verbose=0):
        super().__init__(verbose)
        self.log_interval = log_interval
        self.step_count = 0
        self.episode_count = 0
        self.episode_reward = 0.0  # ì—í”¼ì†Œë“œ ì´ ë³´ìƒ ì¶”ì 
        self.start_exploration_rate = False  # ì„±ê³µ ê¸°ì¤€ì  í”Œë˜ê·¸
        self.initial_exploration_rate = 1.0  # ì´ˆê¸° íƒí—˜ë¥ 

    def _on_step(self) -> bool:
        self.step_count += 1

        # ë²¡í„°í™”ëœ í™˜ê²½ì—ì„œëŠ” dones ë°°ì—´ ì‚¬ìš©
        dones = self.locals.get("dones", [False])

        # print(
        #     f"ğŸ” step: {self.step_count}, episode: {self.episode_count}, íƒí—˜ë¥ : {self.model.exploration_rate:.4f}"
        # )

        # ì—í”¼ì†Œë“œê°€ ëë‚¬ëŠ”ì§€ í™•ì¸ (ë²¡í„°í™”ëœ í™˜ê²½)
        if dones[0]:  # ì²« ë²ˆì§¸ í™˜ê²½ì˜ done ìƒíƒœ
            self.episode_count += 1

            # infosì—ì„œ ì—í”¼ì†Œë“œ ì •ë³´ ì¶”ì¶œ
            episode_info = self.locals.get("infos", [{}])[0].get("episode", {})
            episode_reward = episode_info.get("r", 0.0)  # ì´ ë³´ìƒ
            episode_length = episode_info.get("l", 0)  # ì—í”¼ì†Œë“œ ê¸¸ì´
            episode_time = episode_info.get("t", 0.0)  # ì†Œìš” ì‹œê°„

            # ì—í”¼ì†Œë“œ ì •ë³´ ì¶œë ¥
            # print(
            #     f"ğŸ¯ ì—í”¼ì†Œë“œ {self.episode_count} ì¢…ë£Œ!"
            #     f" ì´ ë³´ìƒ: {episode_reward:.1f},"
            #     f" ê¸¸ì´: {episode_length} ìŠ¤í…,"
            #     f" ì‹œê°„: {episode_time:.3f}ì´ˆ"
            # )

            if self.episode_count % self.log_interval == 0:
                print(
                    f"ğŸ” ì½œë°± ë¡œê¹… - ì—í”¼ì†Œë“œ {self.episode_count}, steps: {self.step_count}, reward: {episode_reward}"
                )
                # print("=" * 50)

                # 1. ê¸°ë³¸ ì •ë³´
                # print(f"ğŸ“Š í˜„ì¬ ìŠ¤í…: {self.num_timesteps}")
                # print(f"ğŸ“Š ì—í”¼ì†Œë“œ ìˆ˜: {self.episode_count}")

                # 2. ë³´ìƒ ì •ë³´ (ë²¡í„°í™”ëœ í™˜ê²½)
                # rewards = self.locals.get("rewards", [0])
                # infos = self.locals.get("infos", [{}])
                # episode_info = infos[0].get("episode", {})

                # if rewards:
                #     print(f"ğŸ’° í˜„ì¬ ë³´ìƒ: {rewards[0]}")
                #     print(f"ğŸ’° ë³´ìƒ ë°°ì—´: {rewards}")

                # if episode_info:
                #     if episode_info.get("r", 0.0) > -200.0:
                #         print(f"ğŸ’° ì—í”¼ì†Œë“œ ì´ ë³´ìƒ: {episode_info.get('r', 0.0):.1f}")
                #         print(f"ğŸ’° ì—í”¼ì†Œë“œ ê¸¸ì´: {episode_info.get('l', 0)} ìŠ¤í…")
                #         print(f"ğŸ’° ì—í”¼ì†Œë“œ ì‹œê°„: {episode_info.get('t', 0.0):.3f}ì´ˆ")

                #         # ì²« ë²ˆì§¸ ì„±ê³µ ì‹œ ê¸°ì¤€ì  ì„¤ì •
                #         if not self.start_exploration_rate:
                #             self.start_exploration_rate = True
                #             print(f"ğŸ¯ ì²« ë²ˆì§¸ ì„±ê³µ! ì—í”¼ì†Œë“œ {self.episode_count} - íƒí—˜ë¥  ì¡°ì • ì‹œì‘")
                #         else:
                #             print(f"ğŸ¯ ì„±ê³µ! ì—í”¼ì†Œë“œ {self.episode_count} - ë³´ìƒ: {episode_info.get('r', 0.0):.1f}")

                #         # ì„±ê³µ í›„ íƒí—˜ë¥  ê°ì†Œ (ì„±ê³µí•œ ì •ì±…ì„ ë” í™œìš©)
                #         if self.start_exploration_rate:
                #             try:
                #                 if hasattr(self.model, "exploration_rate"):
                #                     current_eps = self.model.exploration_rate
                #                     new_eps = max(0.05, current_eps * 0.95)  # 5% ê°ì†Œ
                #                     self.model.exploration_rate = new_eps
                #                     print(f"ğŸ” íƒí—˜ë¥  ê°ì†Œ: {current_eps:.4f} â†’ {new_eps:.4f}")
                #             except:
                #                 print(f"ğŸ” íƒí—˜ë¥  ì¡°ì • ì‹¤íŒ¨")
                # print(f"ğŸ’° ì—í”¼ì†Œë“œ ì´ ë³´ìƒ: {episode_info.get('r', 0.0):.1f}")
                # print(f"ğŸ’° ì—í”¼ì†Œë“œ ê¸¸ì´: {episode_info.get('l', 0)} ìŠ¤í…")
                # print(f"ğŸ’° ì—í”¼ì†Œë“œ ì‹œê°„: {episode_info.get('t', 0.0):.3f}ì´ˆ")

                #     # 3. ëª¨ë¸ ì •ë³´ (ê°„ë‹¨í•˜ê²Œ)
                # print(f"ğŸ¤– ëª¨ë¸: {type(self.model).__name__}")

                # 4. íƒí—˜ ê´€ë ¨ ì •ë³´
                # try:
                #     if hasattr(self.model, "exploration_rate"):
                #         print(f"ğŸ” íƒí—˜ë¥ : {self.model.exploration_rate:.4f}")
                #     if hasattr(self.model, "exploration_initial_eps"):
                #         print(f"ğŸ” ì´ˆê¸° íƒí—˜ë¥ : {self.model.exploration_initial_eps}")
                #         print(f"ğŸ” ìµœì¢… íƒí—˜ë¥ : {self.model.exploration_final_eps}")
                #     else:
                #         print(f"ğŸ” íƒí—˜ë¥ : ì ‘ê·¼ ë¶ˆê°€")
                # except:
                #     print(f"ğŸ” íƒí—˜ë¥ : ì ‘ê·¼ ë¶ˆê°€")

                #     # 5. í•™ìŠµ ê´€ë ¨ ì •ë³´ (ê°„ë‹¨í•˜ê²Œ)
                # try:
                #     if hasattr(self.model, "learning_rate"):
                #         print(f"ğŸ“š í•™ìŠµë¥ : {self.model.learning_rate}")
                #     if hasattr(self.model, "gamma"):
                #         print(f"ğŸ“š gamma: {self.model.gamma}")
                # except:
                #     print(f"ğŸ“š í•™ìŠµ ì •ë³´: ì ‘ê·¼ ë¶ˆê°€")

                # ì¤‘ë³µëœ íƒí—˜ ì •ë³´ ì œê±° (ìœ„ì—ì„œ ì´ë¯¸ ì¶œë ¥ë¨)

                # print("=" * 50)

        return True


def create_env(video_folder=None):
    """CartPole-v1 í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if video_folder:
        env = gym.make("CartPole-v1", render_mode="rgb_array")
        env = RecordVideo(env, video_folder)
        print(f"ë¹„ë””ì˜¤ ë…¹í™”ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤: {video_folder}")
        return env

    env = gym.make("CartPole-v1")
    # env = Monitor(env)
    return env


def setup_logging():
    """ë¡œê·¸ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    return log_dir


def setup_model_save_dir():
    """ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    save_dir = "zips"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    return save_dir


def create_dqn_model(env, log_dir="logs"):
    """DQN ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("DQN ëª¨ë¸ ìƒì„±...")

    model = DQN(
        "MlpPolicy",
        env,
        # Stable Baselines3 DQN ê¸°ë³¸ê°’ë“¤ (n-step ìµœì í™”)
        learning_rate=5e-4,  # ë” ë†’ì€ í•™ìŠµë¥  (ê¸°ë³¸ê°’: 1e-4)
        buffer_size=10000,  # ë” ì‘ì€ ë²„í¼ (ê¸°ë³¸ê°’: 1,000,000)
        learning_starts=32,  # ë¹ ë¥¸ í•™ìŠµ ì‹œì‘ (ê¸°ë³¸ê°’: 100)
        batch_size=32,  # ë” í° ë°°ì¹˜ (ê¸°ë³¸ê°’: 32)
        n_steps=20,
        target_update_interval=300,  # ë§¤ìš° ë¹ ë¥¸ ì—…ë°ì´íŠ¸ (ê¸°ë³¸ê°’: 10000)
        exploration_fraction=0.1,  # ë” ê¸´ íƒí—˜ ê¸°ê°„ (ê¸°ë³¸ê°’: 0.1)
        exploration_initial_eps=1.0,  # ê¸°ë³¸ê°’: 1.0
        exploration_final_eps=0.05,  # ë” ë†’ì€ ìµœì¢… íƒí—˜ë¥  (ê¸°ë³¸ê°’: 0.05)
        tensorboard_log=os.path.join(log_dir, "dqn"),  # ê¸°ë³¸ê°’: None
        policy_kwargs={
            "net_arch": [256, 256],  # samplecode.pyì™€ ë¹„ìŠ·í•œ êµ¬ì¡°
            "activation_fn": torch.nn.ReLU,  # í™œì„±í™” í•¨ìˆ˜
        },
        device="cuda",  # GPU ì‚¬ìš© (CUDA 12.8 ì„¤ì¹˜ë¨)
    )

    return model


def evaluate_model(model, env, n_eval_episodes=10):
    """ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    mean_reward, std_reward = evaluate_policy(
        model, env, n_eval_episodes=n_eval_episodes
    )
    print(f"í‰ê·  ë³´ìƒ: {mean_reward:.2f} +/- {std_reward:.2f}")
    return mean_reward, std_reward


def test_model(model, env, n_episodes=5):
    """í›ˆë ¨ëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    print(f"\n{model.__class__.__name__} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    episode_rewards = []
    episode_steps = []
    clear_count = 0

    for episode in range(n_episodes):
        obs, _ = env.reset()
        done = False
        truncated = False
        total_reward = 0
        steps = 0

        while not (done or truncated):
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = env.step(action)
            total_reward += reward
            steps += 1

            if steps >= 500:  # CartPole-v1ì˜ ìµœëŒ€ ìŠ¤í…
                break

        episode_rewards.append(total_reward)
        episode_steps.append(steps)

        print(f"ì—í”¼ì†Œë“œ {episode + 1}: ì´ ë³´ìƒ = {total_reward}, ìŠ¤í… = {steps}")

        if total_reward >= 475:  # CartPole-v1 í´ë¦¬ì–´ ê¸°ì¤€ (500 ìŠ¤í… ì¤‘ 475 ì´ìƒ)
            print(f"ğŸ‰ ì—í”¼ì†Œë“œ {episode + 1}ì—ì„œ CartPole-v1ì„ í´ë¦¬ì–´í–ˆìŠµë‹ˆë‹¤!")
            clear_count += 1
        else:
            print(f"âŒ ì—í”¼ì†Œë“œ {episode + 1}ì—ì„œ í´ë¦¬ì–´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # í†µê³„ ìš”ì•½
    avg_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)
    clear_rate = clear_count / n_episodes * 100

    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    print(f"í‰ê·  ë³´ìƒ: {avg_reward:.2f} +/- {std_reward:.2f}")
    print(f"í´ë¦¬ì–´ìœ¨: {clear_rate:.1f}% ({clear_count}/{n_episodes})")
    print(f"ìµœê³  ë³´ìƒ: {max(episode_rewards)}")
    print(f"ìµœì € ë³´ìƒ: {min(episode_rewards)}")

    return episode_rewards, episode_steps


def run_dqn(no_save=False):
    """DQN ì•Œê³ ë¦¬ì¦˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    env = create_env()
    log_dir = setup_logging()
    save_dir = setup_model_save_dir()

    print(f"\n{'='*50}")
    print("DQN ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸")
    print(f"{'='*50}")

    # ëª¨ë¸ ìƒì„±
    model = create_dqn_model(env, log_dir)

    # í•™ìŠµ ì‹¤í–‰
    print("DQN ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    callback = LoggingCallback()
    model.learn(
        total_timesteps=20000,
        log_interval=100,
        callback=callback,
    )

    # í‰ê°€
    mean_reward, std_reward = evaluate_model(model, env, n_eval_episodes=20)

    # í…ŒìŠ¤íŠ¸
    test_model(model, env, n_episodes=3)

    # ëª¨ë¸ ì €ì¥
    if not no_save:
        model_path = os.path.join(save_dir, "cartpole_dqn")
        model.save(model_path)
        print(f"DQN ëª¨ë¸ì´ {model_path}.zipìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ëª¨ë¸ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    return mean_reward, std_reward


def play_dqn():
    """ì €ì¥ëœ DQN ëª¨ë¸ë¡œ ë¹„ë””ì˜¤ë¥¼ ì°ìŠµë‹ˆë‹¤."""
    save_dir = setup_model_save_dir()
    model_path = os.path.join(save_dir, "cartpole_dqn")

    # ëª¨ë¸ ë¡œë“œ
    try:
        model = DQN.load(model_path)
        print(f"ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {model_path}")
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œì£¼ì„¸ìš”.")
        return

    # ë¹„ë””ì˜¤ í´ë” ì„¤ì •
    video_folder = "videos/dqn"
    if not os.path.exists(video_folder):
        os.makedirs(video_folder)

    # ë¹„ë””ì˜¤ ë…¹í™” í™˜ê²½ ìƒì„±
    env = create_env(video_folder=video_folder)

    print(f"\n{'='*50}")
    print("DQN ëª¨ë¸ ë¹„ë””ì˜¤ ë…¹í™” ì‹œì‘")
    print(f"{'='*50}")

    # ë¹„ë””ì˜¤ ë…¹í™” ì‹¤í–‰
    obs, _ = env.reset()
    done = False
    truncated = False
    total_reward = 0.0
    steps = 0

    while not (done or truncated):
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, done, truncated, info = env.step(action)
        total_reward += float(reward)
        steps += 1

        if steps >= 500:  # CartPole-v1ì˜ ìµœëŒ€ ìŠ¤í…
            break

    env.close()

    print(f"ë¹„ë””ì˜¤ ë…¹í™” ì™„ë£Œ!")
    print(f"ì´ ë³´ìƒ: {total_reward}, ìŠ¤í…: {steps}")
    print(f"ë¹„ë””ì˜¤ íŒŒì¼ ìœ„ì¹˜: {video_folder}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser()
    parser.add_argument("video", nargs="?", default=None)
    parser.add_argument("--no-save", action="store_true")
    args = parser.parse_args()

    if args.video:
        play_dqn()
    else:
        mean_reward, std_reward = run_dqn(no_save=args.no_save)
        print(f"\nğŸ† DQN ìµœì¢… ê²°ê³¼: {mean_reward:.2f} +/- {std_reward:.2f}")


if __name__ == "__main__":
    main()
